---
title: "Churn_NeuralNetworks"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For the neural networks chapter we are going to continue using Churn data, or customer churn. Before loading the data, we invoke the Tidyverse library. After loading the data, we cleaned it up a bit and took a look at it.

```{r, message= FALSE, warning = FALSE}
library(tidyverse)

data <- read_csv("Churn_Modelling.csv") %>% 
  mutate(is_female = ifelse(Gender == "Female",1,0),
         Exited = as.factor(Exited)) %>% 
        select(-RowNumber, -Surname, -Geography, -Gender, -CustomerId) %>% 
  relocate(Exited)
 
data %>% glimpse()
```

We are going to implement the RRNN using the tidymodels library, to maintain the syntax that we have used until now

```{r, message= FALSE, warning=FALSE}
library(tidymodels)
library(discrim) 
set.seed(42)
data_split <- initial_split(data, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)

nrow(test_data)
train_data %>% nrow()
```

We will also use the fitea function that we created in the last class

```{r}
fitea <- function(mod){
  
  modelo_fit <- 
  workflow() %>% 
  add_model(mod) %>% 
  add_recipe(receta) %>% 
  fit(data = train_data)

model_pred <- 
  predict(modelo_fit, test_data, type = "prob") %>% 
  bind_cols(test_data) 

return(model_pred %>% 
  roc_auc(truth = Exited, .pred_0))
}
```

Now we create the recipe, the same as the previous cases, and finally the multi-layer perceptron (mlp) model.

```{r}
receta <- 
  recipe(Exited ~ ., data = train_data)

receta

modelo <- mlp(hidden_units  = 5) %>% 
  set_engine("nnet") %>% 
  set_mode("classification") %>% 
  translate()


fitea(modelo)
```

We see the AUC is very poor. This is because neural networks need the input data to be scaled between -1 and 1, given the nature of their activation functions. we added the scaling steps in the recipe and tested the same model.

```{r}
receta <- 
  recipe(Exited ~ ., data = train_data) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())

receta

modelo <- mlp(hidden_units  = 5) %>% 
  set_engine("nnet") %>% 
  set_mode("classification") %>% 
  translate()


fitea(modelo)
```

The AUC improved quite a bit, now we will see if the number of neurons allows us to improve the result obtained so far. For this, we will create a function that fits the model depending on the number of neurons in the hidden layer.

```{r}
fiteaNN <- function(hid){
  
  mod <- mlp(hidden_units  = hid) %>% 
  set_engine("nnet") %>% 
  set_mode("classification") %>% 
  translate()
  
  modelo_fit <- 
  workflow() %>% 
  add_model(mod) %>% 
  add_recipe(receta) %>% 
  fit(data = train_data)

model_pred <- 
  predict(modelo_fit, test_data, type = "prob") %>% 
  bind_cols(test_data) 

return(model_pred %>% 
  roc_auc(truth = Exited, .pred_0))
}


fiteaNN(5)
fiteaNN(6)
fiteaNN(7)
fiteaNN(8)
fiteaNN(9)
fiteaNN(10)
```

We see that a model with 5 hidden layers obtains better results than other more complex models, so we should preserve this model since it has fewer parameters and therefore is more stable.

For more complex network configurations we should use the tensorflow library, which I will leave as homework

```{r}
library(tensorflow)
# https://colorado.rstudio.com/rsc/churn/modeling/tensorflow-w-r.nb.html
```