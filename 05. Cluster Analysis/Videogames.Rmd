---
title: "Videogames"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For the cluster analysis, we are going to analyze video game sales data, we will take a look at the variables present.

```{r}
library(tidyverse)

videogames  <- read.csv("video_games_sales.csv")

summary(videogames)
```

To cluster, we are going to select the sales variables and evaluate each videogame. To analyze the behavior we are going to exclude global sales since it is a variable that is linearly dependent on the rest of the sales.

Before clustering we must prepare the data:

- Convert User_score to numeric.
- Removing missing data.
- Scale data

```{r}
# Convert User_score to numeric
videogames$User_Score <- as.numeric(videogames$User_Score)

# Removing missing data and Global_Sales
videogames_cleaned <- videogames %>% filter(!(is.na(Critic_Score) | is.na(User_Score))) %>% select(-Global_Sales)

# Scale data
videogames_num <- videogames_cleaned[,c(6:9, 10, 12)]
videogames_scaled = scale(videogames_num) %>% as_tibble()
videogames_scaled %>% summary()
```

## Kmeans

We already have the data scaled, we are going to apply the kmeans algorithm, which is implemented in R base.

To test, let's apply kmeans with k = 10

```{r}
kmeans_model <- kmeans(videogames_scaled, centers = 10)

# create the cluster variable in the videogames_scaled table
videogames_scaled$clus <- kmeans_model$cluster %>% as.factor()

ggplot(videogames_scaled, aes(Critic_Score, User_Score, color=clus)) +
  geom_point(alpha=0.5, show.legend = F) +
  theme_bw()
```

### Intra-cluster Sum of Squares (SSE)

Let's see how the intra-cluster sum of squares evolves as we increase the number of k

```{r, warning=FALSE}
SSinterior <- numeric(50)

for(k in 1:50){
  model <- kmeans(videogames_scaled, centers = k)
  SSinterior[k] <- model$tot.withinss
}

plot(SSinterior)
```

## Evaluation

There are various methods for evaluating the quality of the resulting clusters.

### Visual Inspection

take distance matrices. Sort them according to clusters, rows and columns and display the matrix as image.

```{r}
# use euclidean distance
tempDist <- dist(videogames_num) %>% as.matrix()

# reorder rows and columns based on the cluster obtained
index <- sort(kmeans_model$cluster, index.return=TRUE)
tempDist <- tempDist[index$ix,index$ix]
rownames(tempDist) <- c(1:nrow(videogames_cleaned))
colnames(tempDist) <- c(1:nrow(videogames_cleaned))

image(tempDist)
```

### Hopkins Statistic

The Hopkins statistic is a measure used in clustering analysis to evaluate the grouping of data. It is used to determine whether a data set has a clustered structure or whether the data points are randomly distributed.

When to use the Hopkins statistic:

You should use the Hopkins statistic when you want to evaluate whether your data is suitable for applying clustering algorithms, such as k-means, or whether there is an inherent clustering structure in your data. This is especially useful in situations where you have no prior information about the grouping structure of your data and want to determine whether clustering is appropriate.

How to interpret the Hopkins statistic:

The Hopkins statistic varies in the range from 0 to 1, where:

- Close to 0: Indicates that the data is more evenly distributed and is less likely to have a significant clustering structure.
- Close to 1: Indicates that the data has a significant clustering structure and is suitable for applying clustering algorithms.

The general interpretation is that the closer the value of the Hopkins statistic is to 1, the greater the probability that your data has a clustering structure. On the other hand, if the value is close to 0, it is more likely that the data is evenly distributed and there is no clear clustering structure.

The precise interpretation may vary depending on the context and specific application. A typical reference value is 0.5; If the Hopkins statistic is much greater than 0.5, it suggests a clustering structure, while if it is much less than 0.5, it suggests a more uniform distribution.

```{r, message=FALSE, warning=FALSE}
library(factoextra)

res <- get_clust_tendency(videogames_num, n = 30, graph = FALSE)

print(res)
```


### Correlation Index

Is used to measure the relationship between two variables in a data set. It can be the Pearson correlation coefficient (which measures linear correlation) or the Spearman correlation coefficient (which measures monotonic correlation).

The correlation index varies between -1 and 1:

- If it is close to 1, it indicates a strong positive correlation between the variables (as one increases, the other also tends to increase).
- If it is close to -1, it indicates a strong negative correlation (as one increases, the other tends to decrease).
- If it is close to 0, it indicates a weak or non-existent correlation between the variables.

The correlation index is used to understand the relationship between two variables and can help make data-driven decisions, identify patterns, and predict behaviors.

```{r}
# Correlation
# build an ideal correlation matrix (each entity correlates 1 with its cluster)
tempMatrix <- matrix(0, nrow = nrow(videogames_scaled), ncol = nrow(videogames_scaled))
tempMatrix[which(index$x==1), which(index$x==1)]  <- 1
tempMatrix[which(index$x==2), which(index$x==2)]  <- 1
tempMatrix[which(index$x==3), which(index$x==3)]  <- 1
tempMatrix[which(index$x==4), which(index$x==4)]  <- 1
tempMatrix[which(index$x==5), which(index$x==5)]  <- 1
tempMatrix[which(index$x==6), which(index$x==6)]  <- 1
tempMatrix[which(index$x==7), which(index$x==7)]  <- 1
tempMatrix[which(index$x==8), which(index$x==8)]  <- 1
tempMatrix[which(index$x==9), which(index$x==9)]  <- 1
tempMatrix[which(index$x==10), which(index$x==10)] <- 1

# construct dissimilarity matrix
tempDist2 <- 1/(1+tempDist)

# Calculate correlation 
cor <- cor(tempMatrix[upper.tri(tempMatrix)],tempDist2[upper.tri(tempDist2)])

print(cor)
```

### Cohesion and separation index

Cohesion and separation indices are measures used in the context of clustering analysis to evaluate the quality of groups formed by a clustering algorithm. Below I provide a brief description of each one:

Cohesion Index: This index measures how close the points are within the same group or cluster. The higher the cohesion index, the better the cohesion within groups, meaning that points within the same group are closer to each other. High cohesion is desired in clustering, since it indicates that the points in a group are similar to each other.

Separation Index: This index measures how far points are from different groups or clusters. The higher the separation index, the better the separation between the groups, meaning that the groups are well differentiated and do not overlap with each other. High separation is desired in clustering, as it indicates that the groups are distinct and easily distinguishable.

These indices are used to evaluate the quality of the clustering results. In general, we seek to maximize the cohesion index and minimize the separation index to obtain cohesive and well-separated groups. However, choosing a clustering algorithm and interpreting these indices may depend on the specific nature of your data and your analysis objectives.

```{r, message=FALSE, warning=FALSE}
library(flexclust) # usaremos la distancia implementada en flexclus (dist2) que maneja mejor objetos de diferente tamaÃ±o
data_escala <- apply(videogames_scaled,2,as.numeric)
 
#Cohesion
withinCluster <- numeric(10)
for (i in 1:10){
  tempData <- data_escala[which(kmeans_model$cluster == i),]
  withinCluster[i] <- sum(dist2(tempData,colMeans(tempData))^2)
}
cohesion = sum(withinCluster)
#es equivalente a model$tot.withinss en k-means
print(c(cohesion, kmeans_model$tot.withinss))

#Separation
meanData <- colMeans(data_escala)
SSB <- numeric(10)
for (i in 1:10){
  tempData <- data_escala[which(kmeans_model$cluster==i),]
  SSB[i] <- nrow(tempData)*sum((meanData-colMeans(tempData))^2)
}
separation = sum(SSB)

print(separation)
```

### Silhouette Coefficient

The silhouette coefficient (silhouette coefficient in English) is another measure used to evaluate the quality of the groups formed by a clustering algorithm. Provides a score that indicates how similar each data object is to its own group (cohesion) compared to other groups (separation). It is a metric that varies between -1 and 1, where:

- A value close to 1 indicates that the object is well within its own group and far from other groups, suggesting that grouping is appropriate.
- A value close to 0 indicates that the object is near the boundary between two groups or could be on a boundary between groups.
- A value close to -1 indicates that the object might have been incorrectly assigned to one group and would be closer to another group.

```{r}
library(cluster)

coefSil <- silhouette(kmeans_model$cluster,dist(videogames_scaled))
summary(coefSil)

#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSil) + coord_flip()
```

We use the silhouette coefficient to find the best value of K

```{r, message=FALSE, warning=FALSE}

coefSil=numeric(30)
for (k in 2:30){
  modelo <- kmeans(videogames_scaled, centers = k)
  temp <- silhouette(modelo$cluster,dist(videogames_scaled))
  coefSil[k] <- mean(temp[,3])
}
tempDF=data.frame(CS=coefSil,K=c(1:30))

ggplot(tempDF, aes(x=K, y=CS)) + 
  geom_line() +
  scale_x_continuous(breaks=c(1:30))

```

